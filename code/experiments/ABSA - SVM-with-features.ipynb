{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "# import tensorflow as tf\n",
    "# from tensorflow import keras\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import feature_extraction, model_selection, metrics\n",
    "import numpy as np\n",
    "import string\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import collections\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support\n",
    "import networkx as nx\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Enable eager execution since this code is experimental\n",
    "# tf.enable_eager_execution()\n",
    "# tfe = tf.contrib.eager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "table = str.maketrans('', '', '!\"#$%&\\'()*+,-.:;<=>?@[\\\\]^_`{|}~' + \"0123456789\")\n",
    "table2 = str.maketrans('/', ' ')\n",
    "stopWords = set(stopwords.words('english'))\n",
    "dataset = []\n",
    "sentences = []\n",
    "maxSentLen = 0\n",
    "avgSentLen = 0\n",
    "missingVectors = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2vec = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# with open('stopwords.txt') as file:\n",
    "#     for line in file:\n",
    "#         line = line.lower()\n",
    "#         stopWords.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(text, aspectTerm=None, flag=None):\n",
    "    text = text.replace(\"[comma]\", \"\")\n",
    "    text = text.replace(\"  \", \" \")\n",
    "    text = text.replace('\"', '').replace('.','').replace('(',' ').replace('(','').replace(')','').replace('!','').replace(\"?\",'').replace(\"  \",\" \")\n",
    "    text = text.lstrip('- ')\n",
    "    text = text.lstrip('_')    \n",
    "    text = text.lstrip('_ ')\n",
    "    text = text.lstrip('-')\n",
    "    text = text.rstrip(' ')\n",
    "    text = text.translate(table2)\n",
    "    gw = lambda w: w.lower() if w.lower() in aspectTerm else w.translate(table).lower()\n",
    "    if flag:\n",
    "        x = aspectTerm.replace(\" \",\"_\")\n",
    "        text = text.replace(aspectTerm,x)\n",
    "        temp = text.split()\n",
    "        try:\n",
    "            atPos = temp.index(x)\n",
    "        except ValueError:\n",
    "            xx = x[0].replace(\"_\", \"\")\n",
    "            for w in temp:\n",
    "                if x in w:\n",
    "                    x = w\n",
    "                    atPos = temp.index(x)\n",
    "                    break\n",
    "                if xx in w:\n",
    "                    xx = w\n",
    "                    atPos = temp.index(xx)\n",
    "        return [text, x+'-'+str(atPos)]\n",
    "    elif aspectTerm:\n",
    "        text = [gw(word) for word in text.split() \n",
    "                if len(word) > 1 and (word.lower() in aspectTerm or word.translate(table).lower() not in stopWords)]\n",
    "    else:\n",
    "        text = [word.lower() for word in text.translate(table2).split() if len(word) > 1]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# should $numbers be removed ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "length = 0\n",
    "ll = []\n",
    "with open('data-2_train.csv') as file:\n",
    "    file.readline()\n",
    "    for line in file:\n",
    "        data = line.split(\",\")\n",
    "        data[2] = preprocess(data[2])\n",
    "        data[1] = preprocess(data[1], data[2])\n",
    "        sentences.append(preprocess(' '.join(data[1]), ' '.join(data[2]), True))\n",
    "        data[-1] = data[-1].strip()\n",
    "        length += len(data[1])\n",
    "        ll.append(len(data[1]))\n",
    "        if len(data[1]) > maxSentLen:\n",
    "            maxSentLen = len(data[1])\n",
    "        dataset.append(data)\n",
    "avgSentLen = length / len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36 9.352026651860077\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "print(maxSentLen, avgSentLen)\n",
    "print(sorted(ll)[int(len(ll)/2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(len(ll)), sorted(ll))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "maxSentLen = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weD = 300\n",
    "wvD = weD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def train(data, label):\n",
    "#     clf=LinearSVC(multi_class='crammer_singer',random_state=0)\n",
    "    \n",
    "#     preds = model_selection.cross_val_predict(clf, data, label, cv=10)\n",
    "#     accScore = metrics.accuracy_score(label,preds)\n",
    "#     labels = [-1, 0, 1]\n",
    "#     precision = metrics.precision_score(label,preds,average=None,labels=labels)\n",
    "#     recall = metrics.recall_score(label,preds,average=None,labels=labels)\n",
    "#     f1Score = metrics.f1_score(label,preds,average=None,labels=labels)\n",
    "#     print(\"\\nOverall Acurracy - SVM: \",accScore,\"\\n\")\n",
    "#     for i in range(len(labels)):\n",
    "#         print(\"Precision of %s class: %f\" %(labels[i],precision[i]))\n",
    "#         print(\"Recall of %s class: %f\" %(labels[i],recall[i]))\n",
    "#         print(\"F1-Score of %s class: %f\" %(labels[i],f1Score[i]),\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getSentGraph(sent):\n",
    "    document = nlp(sent)\n",
    "\n",
    "    # Load spacy's dependency tree into a networkx graph\n",
    "    edges = []\n",
    "    for token in document:\n",
    "        # FYI https://spacy.io/docs/api/token\n",
    "        for child in token.children:\n",
    "            edges.append(('{0}-{1}'.format(token.lower_,token.i),\n",
    "                          '{0}-{1}'.format(child.lower_,child.i)))\n",
    "\n",
    "    return nx.Graph(edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X1 = []\n",
    "y = []\n",
    "yForSk = []\n",
    "flag = \"input\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i, data in enumerate(dataset):\n",
    "    data1 = []\n",
    "    meanAspVec = None\n",
    "    \n",
    "    #print(i)\n",
    "    graph = getSentGraph(sentences[i][0])\n",
    "    \n",
    "    #if more than one word in aspect term, take mean\n",
    "    mean = np.zeros((weD, 1))\n",
    "    for w in data[2]:\n",
    "        twv = None\n",
    "        try:\n",
    "            if w in missingVectors:\n",
    "                twv = missingVectors[w]\n",
    "            else:\n",
    "                twv = word2vec[w].reshape(weD, 1)\n",
    "        except KeyError:\n",
    "            twv = np.random.normal(size=(weD, 1))/math.sqrt(weD+1)\n",
    "            missingVectors[w] = twv\n",
    "        mean += twv\n",
    "    meanAspVec = mean/len(data[2])\n",
    "    \n",
    "    E = []\n",
    "    \n",
    "#     for wv in data1:\n",
    "#         E.append(np.dot(wv.T, meanAspVec)/(np.linalg.norm(wv)) * np.linalg.norm(meanAspVec))\n",
    "    \n",
    "#     E = np.array(E).reshape(300, 1)\n",
    "#     A = np.exp(E) / np.sum(np.exp(E))\n",
    "\n",
    "    for w in data[1]:\n",
    "        twv = None\n",
    "        try:\n",
    "            if w in missingVectors:\n",
    "                twv = missingVectors[w]\n",
    "            else:\n",
    "                twv = word2vec[w].reshape(weD, 1)\n",
    "        except KeyError:\n",
    "            twv = np.random.normal(size=(weD, 1))/math.sqrt(weD+1)\n",
    "            missingVectors[w] = twv\n",
    "        E.append(np.dot(twv.T, meanAspVec) / (np.linalg.norm(twv) * np.linalg.norm(meanAspVec)))\n",
    "\n",
    "    A = np.exp(E) / np.sum(np.exp(E)) # softmax\n",
    "    \n",
    "    tttemp = sentences[i][0].split()\n",
    "    distVec = []\n",
    "    for jj, w in enumerate(data[1]):\n",
    "        if w in sentences[i][1]:\n",
    "            dist = 1\n",
    "        else:\n",
    "            try:\n",
    "                dist = 1/nx.shortest_path_length(graph, source=sentences[i][1], target=w+'-'+str(tttemp.index(w)))\n",
    "            except (nx.NetworkXNoPath, KeyError, nx.NetworkXError):\n",
    "                dist = 1/maxSentLen\n",
    "        distVec.append(dist)\n",
    "        twv = None\n",
    "        if w in missingVectors:\n",
    "            twv = missingVectors[w]\n",
    "        else:\n",
    "            twv = word2vec[w].reshape(weD, 1)\n",
    "        if flag == \"attention\":\n",
    "            data1.append(A[jj] * twv)\n",
    "        elif flag == \"input\":\n",
    "            data1.append(twv)\n",
    "\n",
    "#     for w in data[1]:\n",
    "#         twv = None\n",
    "#         try:\n",
    "#             if w in missingVectors:\n",
    "#                 twv = missingVectors[w]\n",
    "#             else:\n",
    "#                 twv = word2vec[w].reshape(300, 1)\n",
    "#         except KeyError:\n",
    "#             twv = np.random.normal(size=(300, 1))/math.sqrt(301)\n",
    "#             missingVectors[w] = twv\n",
    "            \n",
    "#         data1.append(np.vstack((twv, meanAspVec)))\n",
    "    \n",
    "    if len(data1) < maxSentLen:\n",
    "        j = len(data1) + 1\n",
    "        while j <= maxSentLen:\n",
    "            #data1.append(np.vstack((word2vec['#'].reshape(300, 1), meanAspVec)))\n",
    "            #data1.append(np.vstack((np.zeros((300, 1)), meanAspVec)))\n",
    "            data1.append(np.zeros((wvD, 1)))\n",
    "            distVec.append(0.0)\n",
    "            j += 1\n",
    "    \n",
    "    if len(data1) > maxSentLen:\n",
    "        del data1[maxSentLen:]\n",
    "        del distVec[maxSentLen:]\n",
    "    \n",
    "    protoVec = np.array([np.mean(data1, axis=0)]).reshape(300,)\n",
    "    protoVec = np.append(protoVec, distVec)\n",
    "    X1.append(protoVec)\n",
    "    yForSk.append(data[-1])\n",
    "    if data[-1] == '-1':\n",
    "        y.append(np.array([[0.0, 0.0, 1.0]]))\n",
    "    elif data[-1] == '0':\n",
    "        y.append(np.array([[0.0, 1.0, 0.0]]))\n",
    "    elif data[-1] == '1':\n",
    "        y.append(np.array([[1.0, 0.0, 0.0]]))\n",
    "\n",
    "X1 = np.array(X1)\n",
    "y = np.array(y)\n",
    "yForSk = np.array(yForSk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pizza best like thin_crusted_pizza', 'thin_crusted_pizza-3']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[26]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3602, 320)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3602, 1, 3)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['-1', '1', '1']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(yForSk[[0, 1, 2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kf = StratifiedKFold(n_splits=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train(X1, yForSk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold - 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 39   3  39]\n",
      " [ 15  13  36]\n",
      " [  7   3 207]]\n",
      "(array([ 0.63934426,  0.68421053,  0.73404255]), array([ 0.48148148,  0.203125  ,  0.95391705]), array([ 0.54929577,  0.31325301,  0.82965932]), array([ 81,  64, 217], dtype=int64))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\numpy\\lib\\arraysetops.py:391: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask &= (ar1 != a)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy= 0.71546961326 precision= [ 0.73404255  0.68421053  0.63934426] recall= [ 0.95391705  0.203125    0.48148148] F1 Score= [ 0.82965932  0.31325301  0.54929577]\n",
      "Fold - 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 44   9  28]\n",
      " [ 11  10  43]\n",
      " [ 14   5 198]]\n",
      "(array([ 0.63768116,  0.41666667,  0.73605948]), array([ 0.54320988,  0.15625   ,  0.9124424 ]), array([ 0.58666667,  0.22727273,  0.81481481]), array([ 81,  64, 217], dtype=int64))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\numpy\\lib\\arraysetops.py:391: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask &= (ar1 != a)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy= 0.696132596685 precision= [ 0.73605948  0.41666667  0.63768116] recall= [ 0.9124424   0.15625     0.54320988] F1 Score= [ 0.81481481  0.22727273  0.58666667]\n",
      "Fold - 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 30   4  47]\n",
      " [ 15  12  37]\n",
      " [ 12   1 204]]\n",
      "(array([ 0.52631579,  0.70588235,  0.70833333]), array([ 0.37037037,  0.1875    ,  0.94009217]), array([ 0.43478261,  0.2962963 ,  0.80792079]), array([ 81,  64, 217], dtype=int64))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\numpy\\lib\\arraysetops.py:391: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask &= (ar1 != a)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy= 0.67955801105 precision= [ 0.70833333  0.70588235  0.52631579] recall= [ 0.94009217  0.1875      0.37037037] F1 Score= [ 0.80792079  0.2962963   0.43478261]\n",
      "Fold - 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 43   6  32]\n",
      " [ 14   9  40]\n",
      " [  6   2 209]]\n",
      "(array([ 0.68253968,  0.52941176,  0.74377224]), array([ 0.5308642 ,  0.14285714,  0.96313364]), array([ 0.59722222,  0.225     ,  0.83935743]), array([ 81,  63, 217], dtype=int64))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\numpy\\lib\\arraysetops.py:391: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask &= (ar1 != a)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy= 0.722991689751 precision= [ 0.74377224  0.52941176  0.68253968] recall= [ 0.96313364  0.14285714  0.5308642 ] F1 Score= [ 0.83935743  0.225       0.59722222]\n",
      "Fold - 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 48   6  27]\n",
      " [ 16   7  40]\n",
      " [ 12   1 203]]\n",
      "(array([ 0.63157895,  0.5       ,  0.75185185]), array([ 0.59259259,  0.11111111,  0.93981481]), array([ 0.61146497,  0.18181818,  0.83539095]), array([ 81,  63, 216], dtype=int64))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\numpy\\lib\\arraysetops.py:391: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask &= (ar1 != a)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy= 0.716666666667 precision= [ 0.75185185  0.5         0.63157895] recall= [ 0.93981481  0.11111111  0.59259259] F1 Score= [ 0.83539095  0.18181818  0.61146497]\n",
      "Fold - 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 36   3  41]\n",
      " [ 20  11  32]\n",
      " [ 15   4 197]]\n",
      "(array([ 0.50704225,  0.61111111,  0.72962963]), array([ 0.45      ,  0.17460317,  0.91203704]), array([ 0.47682119,  0.27160494,  0.81069959]), array([ 80,  63, 216], dtype=int64))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\numpy\\lib\\arraysetops.py:391: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask &= (ar1 != a)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy= 0.679665738162 precision= [ 0.72962963  0.61111111  0.50704225] recall= [ 0.91203704  0.17460317  0.45      ] F1 Score= [ 0.81069959  0.27160494  0.47682119]\n",
      "Fold - 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 45   3  32]\n",
      " [ 18  12  33]\n",
      " [  9   1 206]]\n",
      "(array([ 0.625    ,  0.75     ,  0.7601476]), array([ 0.5625    ,  0.19047619,  0.9537037 ]), array([ 0.59210526,  0.30379747,  0.84599589]), array([ 80,  63, 216], dtype=int64))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\numpy\\lib\\arraysetops.py:391: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask &= (ar1 != a)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy= 0.732590529248 precision= [ 0.7601476  0.75       0.625    ] recall= [ 0.9537037   0.19047619  0.5625    ] F1 Score= [ 0.84599589  0.30379747  0.59210526]\n",
      "Fold - 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 36   5  39]\n",
      " [ 14  11  38]\n",
      " [ 11   1 204]]\n",
      "(array([ 0.59016393,  0.64705882,  0.72597865]), array([ 0.45      ,  0.17460317,  0.94444444]), array([ 0.5106383 ,  0.275     ,  0.82092555]), array([ 80,  63, 216], dtype=int64))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\numpy\\lib\\arraysetops.py:391: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask &= (ar1 != a)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy= 0.699164345404 precision= [ 0.72597865  0.64705882  0.59016393] recall= [ 0.94444444  0.17460317  0.45      ] F1 Score= [ 0.82092555  0.275       0.5106383 ]\n",
      "Fold - 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 32   8  40]\n",
      " [ 12  12  39]\n",
      " [  9   1 206]]\n",
      "(array([ 0.60377358,  0.57142857,  0.72280702]), array([ 0.4       ,  0.19047619,  0.9537037 ]), array([ 0.48120301,  0.28571429,  0.82235529]), array([ 80,  63, 216], dtype=int64))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\numpy\\lib\\arraysetops.py:391: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask &= (ar1 != a)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy= 0.696378830084 precision= [ 0.72280702  0.57142857  0.60377358] recall= [ 0.9537037   0.19047619  0.4       ] F1 Score= [ 0.82235529  0.28571429  0.48120301]\n",
      "Fold - 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 38   6  36]\n",
      " [ 15  11  37]\n",
      " [  4   6 206]]\n",
      "(array([ 0.66666667,  0.47826087,  0.73835125]), array([ 0.475     ,  0.17460317,  0.9537037 ]), array([ 0.55474453,  0.25581395,  0.83232323]), array([ 80,  63, 216], dtype=int64))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\numpy\\lib\\arraysetops.py:391: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask &= (ar1 != a)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy= 0.710306406685 precision= [ 0.73835125  0.47826087  0.66666667] recall= [ 0.9537037   0.17460317  0.475     ] F1 Score= [ 0.83232323  0.25581395  0.55474453]\n"
     ]
    }
   ],
   "source": [
    "accuracies = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "f1s = []\n",
    "\n",
    "for index, (train_ind, test_ind) in enumerate(kf.split(X1, yForSk)):\n",
    "    print(\"Fold -\", index)\n",
    "    xtrain, xtest = X1[train_ind], X1[test_ind]\n",
    "    ytrain, ytest = list(yForSk[train_ind]), list(yForSk[test_ind])\n",
    "    \n",
    "    clf=LinearSVC(multi_class='crammer_singer',random_state=0)\n",
    "    #print(X_train.toarray())\n",
    "    clf.fit(xtrain, ytrain)\n",
    "    #print(log_reg.score(X_train, y_train))\n",
    "    #print(log_reg.score(X_test, y_test))\n",
    "    ypred = clf.predict(xtest)\n",
    "    acc = np.mean(ytest == ypred)\n",
    "    conf_matrix = confusion_matrix(ytest, ypred)\n",
    "    print(conf_matrix)\n",
    "    #print(X_train.shape, X_train.shape)\n",
    "    print(precision_recall_fscore_support(ytest, ypred))\n",
    "    stats = precision_recall_fscore_support(ytest, ypred, labels=[-1, 0, 1])\n",
    "    \n",
    "    p = np.array([stats[0][2], stats[0][1], stats[0][0]])\n",
    "    r = np.array([stats[1][2], stats[1][1], stats[1][0]])\n",
    "    f = np.array([stats[2][2], stats[2][1], stats[2][0]])\n",
    "    \n",
    "    accuracies.append(acc)\n",
    "    precisions.append(p)\n",
    "    recalls.append(r)\n",
    "    f1s.append(f)\n",
    "    \n",
    "    print(\"accuracy=\", acc, \"precision=\", p, \"recall=\", r, \"F1 Score=\", f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# accuracies = []\n",
    "# precisions = []\n",
    "# recalls = []\n",
    "# f1s = []\n",
    "\n",
    "# for index, (train_ind, test_ind) in enumerate(kf.split(X1, yForSk)):\n",
    "#     print(\"Fold -\", index)\n",
    "#     xtrain, xtest = X1[train_ind], X1[test_ind]\n",
    "#     ytrain, ytest = y[train_ind], y[test_ind]\n",
    "    \n",
    "#     model = train(xtrain, ytrain)\n",
    "    \n",
    "#     predictions = model.predict(xtest)\n",
    "    \n",
    "#     y_pred = tf.argmax(predictions, dimension=1)\n",
    "#     y_test = tf.argmax(ytest.reshape(len(ytest), 3), dimension=1)\n",
    "    \n",
    "#     print(\"Labels=\", np.unique(y_test, return_counts=True))\n",
    "#     print(\"Predictions=\", np.unique(y_pred, return_counts=True))\n",
    "    \n",
    "#     cm = confusion_matrix(y_test, y_pred)\n",
    "#     numer = np.diag(cm)\n",
    "#     numer = numer.astype(np.float64)\n",
    "#     deno = np.sum(cm, axis = 1, dtype=np.float64)\n",
    "#     recall = np.divide(numer, deno, out=np.zeros_like(numer), where=deno!=0)\n",
    "#     deno = np.sum(cm, axis = 0, dtype=np.float64)\n",
    "#     precision = np.divide(numer, deno, out=np.zeros_like(numer), where=deno!=0)\n",
    "#     #loss, acc = model.evaluate(xtest, ytest)\n",
    "#     nnum = (precision*recall)\n",
    "#     ddeno = (precision+recall)\n",
    "#     f1 = 2*np.divide(nnum, ddeno, out=np.zeros_like(nnum), where=ddeno!=0)\n",
    "#     acc = np.sum(numer)/np.sum(deno)\n",
    "    \n",
    "#     accuracies.append(acc)\n",
    "#     precisions.append(precision)\n",
    "#     recalls.append(recall)\n",
    "#     f1s.append(f1)\n",
    "#     print(\"accuracy=\", acc, \"precision=\", precision, \"recall=\", recall, \"F1 Score=\", f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Scores:\n",
      "Avg. accuracy= 0.704892442699\n",
      "Avg. precision= [ 0.73509736  0.58940307  0.61101063]\n",
      "Avg. recall= [ 0.94269927  0.17056052  0.48560185]\n",
      "Avg. f1= [ 0.82594429  0.26355709  0.53949445]\n"
     ]
    }
   ],
   "source": [
    "print(\"Average Scores:\")\n",
    "print(\"Avg. accuracy=\", np.sum(accuracies)/len(accuracies))\n",
    "print(\"Avg. precision=\", np.sum(precisions, axis=0)/len(precisions))\n",
    "print(\"Avg. recall=\", np.sum(recalls, axis=0)/len(recalls))\n",
    "print(\"Avg. f1=\", np.sum(f1s, axis=0)/len(f1s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word2vec['#']#.reshape(300, 1) #numpy array\n",
    "#print(word2vec.similarity('computer', 'laptop'))\n",
    "#np.zeros(300).shape\n",
    "#word2vec.distances(word2vec['computer'], other_words=[\"laptop\"])\n",
    "#np.append(word2vec[\"###GB\"], np.array([1.0]))\n",
    "#\"accordingly\" in stopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tteest(text, aspectTerm):\n",
    "#     aspectTerm = aspectTerm.replace(\"[comma]\", \"\").replace(\"  \", \" \").lower()\n",
    "#     text = text.lower()\n",
    "#     x = aspectTerm.replace(\" \",\"_\")\n",
    "#     text = text.replace('\"', '').replace('.','').replace('(',' ').replace('(','').replace(')','').replace('!','').replace(\"?\",'').replace(\"  \",\" \").replace(aspectTerm,x)\n",
    "#     text = text.lstrip('- ')\n",
    "#     text = text.lstrip('_')    \n",
    "#     text = text.lstrip('_ ')\n",
    "#     text = text.lstrip('-')\n",
    "#     text = text.rstrip(' ')\n",
    "#     atPos = text.split().index(x)\n",
    "#     return [text, x+'-'+str(atPos)]\n",
    "\n",
    "\n",
    "# tteest(\"I also enjoy the fact that my MacBook Pro laptop allows me to run Windows 7 on it by using the VMWare program.\",\"VMWare program\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# oma = []\n",
    "# def chamma(line):\n",
    "#     data = line.split(\",\")\n",
    "#     data[2] = preprocess(data[2])\n",
    "#     #print(data[2][0].replace(\"_\", \"\"))\n",
    "#     print(data[2])\n",
    "#     data[1] = preprocess(data[1], data[2])\n",
    "#     print(data[1])\n",
    "#     oma.append(preprocess(' '.join(data[1]), ' '.join(data[2]), True))\n",
    "\n",
    "# chamma(\"1805_1,Good for every day computing and web browsing.,every day computing,9--28,1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# oma[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
