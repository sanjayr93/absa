{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import feature_extraction, model_selection, metrics\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "import string\n",
    "#import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import collections\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support\n",
    "import networkx as nx\n",
    "import spacy\n",
    "from spacy.symbols import ORTH, LEMMA, POS\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "nlp.tokenizer.add_special_case(\"dont\", [{ORTH: \"dont\", LEMMA: \"dont\", POS: \"neg\"}])\n",
    "nlp.tokenizer.add_special_case(\"didnt\", [{ORTH: \"didnt\", LEMMA: \"didnt\", POS: \"neg\"}])\n",
    "nlp.tokenizer.add_special_case(\"isnt\", [{ORTH: \"isnt\", LEMMA: \"isnt\", POS: \"neg\"}])\n",
    "nlp.tokenizer.add_special_case(\"wasnt\", [{ORTH: \"wasnt\", LEMMA: \"wasnt\", POS: \"neg\"}])\n",
    "nlp.tokenizer.add_special_case(\"doesnt\", [{ORTH: \"doesnt\", LEMMA: \"doesnt\", POS: \"neg\"}])\n",
    "nlp.tokenizer.add_special_case(\"couldnt\", [{ORTH: \"couldnt\", LEMMA: \"couldnt\", POS: \"neg\"}])\n",
    "nlp.tokenizer.add_special_case(\"wouldnt\", [{ORTH: \"wouldnt\", LEMMA: \"wouldnt\", POS: \"neg\"}])\n",
    "nlp.tokenizer.add_special_case(\"cant\", [{ORTH: \"cant\", LEMMA: \"cant\", POS: \"neg\"}])\n",
    "nlp.tokenizer.add_special_case(\"cannot\", [{ORTH: \"cannot\", LEMMA: \"cannot\", POS: \"neg\"}])\n",
    "nlp.tokenizer.add_special_case(\"wont\", [{ORTH: \"wont\", LEMMA: \"wont\", POS: \"neg\"}])\n",
    "nlp.tokenizer.add_special_case(\"havent\", [{ORTH: \"havent\", LEMMA: \"havent\", POS: \"neg\"}])\n",
    "nlp.tokenizer.add_special_case(\"hadnt\", [{ORTH: \"hadnt\", LEMMA: \"hadnt\", POS: \"neg\"}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "table = str.maketrans('', '', '!\"#$%&\\'()*+,-.:;<=>?@[\\\\]^_`{|}~' + \"0123456789\")\n",
    "table2 = str.maketrans('/', ' ')\n",
    "negWords = {'never','but','not','didn\\'t','isn\\'t','wasn\\'t','doesn\\'t','couldn\\'t','didnt','isnt','wasnt','doesnt','couldnt'\n",
    "   ,'wouldn\\'t','wouldnt','don\\'t','dont','can\\'t','cant', 'cannot', 'wont', 'won\\'t', 'havent', 'haven\\'t', 'hadnt', 'hadn\\'t'}\n",
    "stopWords = set(stopwords.words('english')) - negWords\n",
    "dataset = []\n",
    "sentences = []\n",
    "maxSentLen = 0\n",
    "avgSentLen = 0\n",
    "missingVectors = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2vec = TfidfVectorizer(encoding='utf-8', ngram_range=(1, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# with open('stopwords.txt') as file:\n",
    "#     for line in file:\n",
    "#         line = line.lower()\n",
    "#         stopWords.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(text, aspectTerm=None, flag=None):\n",
    "    text = text.replace(\"[comma]\", \"\")\n",
    "    text = text.replace(\"  \", \" \")\n",
    "    text = text.replace('\"', '').replace('.','').replace('(',' ').replace('(','').replace(')','').replace('!','').replace(\"?\",'').replace(\"  \",\" \")\n",
    "    text = text.lstrip('- ')\n",
    "    text = text.lstrip('_')    \n",
    "    text = text.lstrip('_ ')\n",
    "    text = text.lstrip('-')\n",
    "    text = text.rstrip(' ')\n",
    "    text = text.translate(table2)\n",
    "    gw = lambda w: w.lower() if w.lower() in aspectTerm else w.translate(table).lower()\n",
    "    if flag:\n",
    "        x = aspectTerm.replace(\" \",\"_\")\n",
    "        text = text.replace(aspectTerm,x)\n",
    "        temp = text.split()\n",
    "        try:\n",
    "            atPos = temp.index(x)\n",
    "        except ValueError:\n",
    "            xx = x[0].replace(\"_\", \"\")\n",
    "            for w in temp:\n",
    "                if x in w:\n",
    "                    x = w\n",
    "                    atPos = temp.index(x)\n",
    "                    break\n",
    "                if xx in w:\n",
    "                    xx = w\n",
    "                    atPos = temp.index(xx)\n",
    "        return [text, x+'-'+str(atPos)]\n",
    "    elif aspectTerm:\n",
    "        text = [gw(word) for word in text.split() \n",
    "                if len(word) > 1 and (word.lower() in aspectTerm or word.translate(table).lower() not in stopWords)]\n",
    "    else:\n",
    "        text = [word.lower() for word in text.translate(table2).split() if len(word) > 1]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# should $numbers be removed ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "length = 0\n",
    "ll = []\n",
    "with open('data-2_train.csv') as file:\n",
    "    file.readline()\n",
    "    for line in file:\n",
    "        data = line.split(\",\")\n",
    "        data[2] = preprocess(data[2])\n",
    "        data[1] = preprocess(data[1], data[2])\n",
    "        sentences.append(preprocess(' '.join(data[1]), ' '.join(data[2]), True))\n",
    "        data[-1] = data[-1].strip()\n",
    "        length += len(data[1])\n",
    "        ll.append(len(data[1]))\n",
    "        if len(data[1]) > maxSentLen:\n",
    "            maxSentLen = len(data[1])\n",
    "        dataset.append(data)\n",
    "avgSentLen = length / len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xtidf = word2vec.fit_transform(list(np.array(sentences)[:, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3602, 30444)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.0778522959288575"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(ll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36 9.588839533592449\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "print(maxSentLen, avgSentLen)\n",
    "print(sorted(ll)[int(len(ll)/2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(len(ll)), sorted(ll))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "maxSentLen = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weD = 300\n",
    "wvD = weD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "posLex = set()\n",
    "negLex = set()\n",
    "with open('positive-words.txt') as file:\n",
    "    for line in file:\n",
    "        if len(line.strip()) > 0:\n",
    "            posLex.add(line.strip())\n",
    "with open('negative-words.txt') as file:\n",
    "    for line in file:\n",
    "        if len(line.strip()) > 0:\n",
    "            negLex.add(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getSentGraph(sent):\n",
    "    global nlp\n",
    "    document = nlp(sent)\n",
    "\n",
    "    # Load spacy's dependency tree into a networkx graph\n",
    "    edges = []\n",
    "    for token in document:\n",
    "        # FYI https://spacy.io/docs/api/token\n",
    "        for child in token.children:\n",
    "            edges.append(('{0}-{1}'.format(token.lower_,token.i),\n",
    "                          '{0}-{1}'.format(child.lower_,child.i)))\n",
    "\n",
    "    return nx.Graph(edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X1 = []\n",
    "y = []\n",
    "yForSk = []\n",
    "flag = \"input\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# with open('chummada.txt', 'a') as ff:\n",
    "#     for s in sentences:\n",
    "#         ff.write(s[1]+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, data in enumerate(dataset):\n",
    "    data1 = []\n",
    "    meanAspVec = None\n",
    "    \n",
    "    #print(i)\n",
    "    graph = getSentGraph(sentences[i][0])\n",
    "    \n",
    "    #if more than one word in aspect term, take mean\n",
    "#     mean = np.zeros((weD, 1))\n",
    "#     for w in data[2]:\n",
    "#         twv = None\n",
    "#         try:\n",
    "#             if w in missingVectors:\n",
    "#                 twv = missingVectors[w]\n",
    "#             else:\n",
    "#                 twv = word2vec[w].reshape(weD, 1)\n",
    "#         except KeyError:\n",
    "#             twv = np.random.normal(size=(weD, 1))/math.sqrt(weD+1)\n",
    "#             missingVectors[w] = twv\n",
    "#         mean += twv\n",
    "#     meanAspVec = mean/len(data[2])\n",
    "    \n",
    "#     E = []\n",
    "    \n",
    "# #     for wv in data1:\n",
    "# #         E.append(np.dot(wv.T, meanAspVec)/(np.linalg.norm(wv)) * np.linalg.norm(meanAspVec))\n",
    "    \n",
    "# #     E = np.array(E).reshape(300, 1)\n",
    "# #     A = np.exp(E) / np.sum(np.exp(E))\n",
    "\n",
    "#     for w in data[1]:\n",
    "#         twv = None\n",
    "#         try:\n",
    "#             if w in missingVectors:\n",
    "#                 twv = missingVectors[w]\n",
    "#             else:\n",
    "#                 twv = word2vec[w].reshape(weD, 1)\n",
    "#         except KeyError:\n",
    "#             twv = np.random.normal(size=(weD, 1))/math.sqrt(weD+1)\n",
    "#             missingVectors[w] = twv\n",
    "#         E.append(np.dot(twv.T, meanAspVec) / (np.linalg.norm(twv) * np.linalg.norm(meanAspVec)))\n",
    "\n",
    "#     A = np.exp(E) / np.sum(np.exp(E)) # softmax\n",
    "    \n",
    "    tttemp = sentences[i][0].split()\n",
    "    distVec = []\n",
    "    for jj, w in enumerate(data[1]):\n",
    "        if w in sentences[i][1]:\n",
    "            dist = 0\n",
    "#         elif w in negWords:\n",
    "#             if 'not' in w:\n",
    "#                 dist = 1/nx.shortest_path_length(graph, source=sentences[i][1], target='not-'+str(tttemp.index(w)))\n",
    "#             elif 'nt' in w:\n",
    "#                 dist = 1/nx.shortest_path_length(graph, source=sentences[i][1], target='nt-'+str(tttemp.index(w)))\n",
    "#             elif 'n\\'t' in w:\n",
    "#                 dist = 1/nx.shortest_path_length(graph, source=sentences[i][1], target='n\\'t-'+str(tttemp.index(w)))\n",
    "        else:\n",
    "            try:\n",
    "                pD = nx.shortest_path_length(graph, source=sentences[i][1], target=w+'-'+str(tttemp.index(w)))\n",
    "                dist = 1/pD\n",
    "            except (nx.NetworkXNoPath, KeyError, nx.NetworkXError):\n",
    "                dist = 1/maxSentLen\n",
    "        distVec.append(dist)\n",
    "#         twv = None\n",
    "#         if w in missingVectors:\n",
    "#             twv = missingVectors[w]\n",
    "#         else:\n",
    "#             twv = word2vec[w].reshape(weD, 1)\n",
    "#         if flag == \"attention\":\n",
    "#             data1.append(A[jj] * twv)\n",
    "#         elif flag == \"input\":\n",
    "#             data1.append(twv)\n",
    "\n",
    "#     for w in data[1]:\n",
    "#         twv = None\n",
    "#         try:\n",
    "#             if w in missingVectors:\n",
    "#                 twv = missingVectors[w]\n",
    "#             else:\n",
    "#                 twv = word2vec[w].reshape(300, 1)\n",
    "#         except KeyError:\n",
    "#             twv = np.random.normal(size=(300, 1))/math.sqrt(301)\n",
    "#             missingVectors[w] = twv\n",
    "            \n",
    "#         data1.append(np.vstack((twv, meanAspVec)))\n",
    "    \n",
    "    # creating lexicon score with window of 5 to the left and right of aspect\n",
    "    lexScore = 0\n",
    "    window = 5\n",
    "    atPos = int(sentences[i][1].rsplit('-', maxsplit=1)[1])\n",
    "    for lind in range(atPos - window, atPos):\n",
    "        if lind >= 0:\n",
    "            if data[1][lind] in posLex:\n",
    "                lexScore += 1*distVec[lind]\n",
    "            elif data[1][lind] in negLex or data[1][lind] in negWords:\n",
    "                lexScore -= 1*distVec[lind]\n",
    "    \n",
    "    for lind in range(atPos + 1, atPos + window + 1):\n",
    "        if lind >= len(data[1]):\n",
    "            break\n",
    "        else:\n",
    "            if data[1][lind] in posLex:\n",
    "                lexScore += 1*distVec[lind]\n",
    "            elif data[1][lind] in negLex or data[1][lind] in negWords:\n",
    "                lexScore -= 1*distVec[lind]\n",
    "\n",
    "    if len(data[1]) < maxSentLen:\n",
    "        j = len(data[1]) + 1\n",
    "        while j <= maxSentLen:\n",
    "            #data1.append(np.vstack((word2vec['#'].reshape(300, 1), meanAspVec)))\n",
    "            #data1.append(np.vstack((np.zeros((300, 1)), meanAspVec)))\n",
    "            #data1.append(np.zeros((wvD, 1)))\n",
    "            distVec.append(1/maxSentLen)\n",
    "            j += 1\n",
    "    \n",
    "    if len(data[1]) > maxSentLen:\n",
    "        #del data1[maxSentLen:]\n",
    "        del distVec[maxSentLen:]\n",
    "    \n",
    "    \n",
    "#     ctr = 0\n",
    "#     for lind in range(atPos - window, atPos):\n",
    "#         if lind >= 0:\n",
    "#             if data[1][lind] in posLex:\n",
    "#                 lexScore += 1/(window - ctr)\n",
    "#             elif data[1][lind] in negLex or data[1][lind] in negWords:\n",
    "#                 lexScore -= 1/(window - ctr)\n",
    "#         ctr += 1\n",
    "    \n",
    "#     ctr = 1\n",
    "#     for lind in range(atPos + 1, atPos + window + 1):\n",
    "#         if lind >= len(data[1]):\n",
    "#             break\n",
    "#         else:\n",
    "#             if data[1][lind] in posLex:\n",
    "#                 lexScore += 1/ctr\n",
    "#             elif data[1][lind] in negLex or data[1][lind] in negWords:\n",
    "#                 lexScore -= 1/ctr\n",
    "#         ctr += 1\n",
    "    \n",
    "    \n",
    "    #protoVec = np.array([np.mean(data1, axis=0)]).reshape(300,)\n",
    "    \n",
    "    protoVec = np.append(Xtidf[i].A[0], distVec)\n",
    "    protoVec = np.append(protoVec, [lexScore])\n",
    "    \n",
    "    X1.append(protoVec)\n",
    "    yForSk.append(data[-1])\n",
    "    if data[-1] == '-1':\n",
    "        y.append(np.array([[0.0, 0.0, 1.0]]))\n",
    "    elif data[-1] == '0':\n",
    "        y.append(np.array([[0.0, 1.0, 0.0]]))\n",
    "    elif data[-1] == '1':\n",
    "        y.append(np.array([[1.0, 0.0, 0.0]]))\n",
    "\n",
    "X1 = np.array(X1)\n",
    "y = np.array(y)\n",
    "yForSk = np.array(yForSk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['little place cute interior_decor affordable city prices', 'interior_decor-3']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[44]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(Xtidf[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 30444)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtidf[0].A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3602, 30465)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3602, 1, 3)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['-1', '1', '1'], \n",
       "      dtype='<U2')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yForSk[[0, 1, 2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kf = StratifiedKFold(n_splits=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train(X1, yForSk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def printScores(accuracies, precisions, recalls, f1s):\n",
    "    print(\"Average Scores:\")\n",
    "    print(\"Avg. accuracy=\", np.sum(accuracies)/len(accuracies))\n",
    "    print(\"Avg. precision=\", np.sum(precisions, axis=0)/len(precisions))\n",
    "    print(\"Avg. recall=\", np.sum(recalls, axis=0)/len(recalls))\n",
    "    print(\"Avg. f1=\", np.sum(f1s, axis=0)/len(f1s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Scores:\n",
      "Avg. accuracy= 0.730966960851\n",
      "Avg. precision= [ 0.80057947  0.53059129  0.62372409]\n",
      "Avg. recall= [ 0.8932433   0.35233135  0.59259259]\n",
      "Avg. f1= [ 0.84430209  0.42224506  0.60669784]\n"
     ]
    }
   ],
   "source": [
    "#LinearSVC\n",
    "accuracies = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "f1s = []\n",
    "\n",
    "for index, (train_ind, test_ind) in enumerate(kf.split(X1, yForSk)):\n",
    "    #print(\"Fold -\", index)\n",
    "    xtrain, xtest = X1[train_ind], X1[test_ind]\n",
    "    ytrain, ytest = list(yForSk[train_ind]), list(yForSk[test_ind])\n",
    "    \n",
    "    clf=LinearSVC(C=1.0, multi_class='crammer_singer', max_iter=1000)\n",
    "    #print(X_train.toarray())\n",
    "    clf.fit(xtrain, ytrain)\n",
    "    #print(log_reg.score(X_train, y_train))\n",
    "    #print(log_reg.score(X_test, y_test))\n",
    "    ypred = clf.predict(xtest)\n",
    "    acc = np.mean(ytest == ypred)\n",
    "    conf_matrix = confusion_matrix(ytest, ypred)\n",
    "    #print(conf_matrix)\n",
    "    #print(X_train.shape, X_train.shape)\n",
    "    #print(precision_recall_fscore_support(ytest, ypred))\n",
    "    stats = precision_recall_fscore_support(ytest, ypred, labels=['-1', '0', '1'])\n",
    "    \n",
    "    p = np.array([stats[0][2], stats[0][1], stats[0][0]])\n",
    "    r = np.array([stats[1][2], stats[1][1], stats[1][0]])\n",
    "    f = np.array([stats[2][2], stats[2][1], stats[2][0]])\n",
    "    \n",
    "    accuracies.append(acc)\n",
    "    precisions.append(p)\n",
    "    recalls.append(r)\n",
    "    f1s.append(f)\n",
    "    \n",
    "    #print(\"accuracy=\", acc, \"precision=\", p, \"recall=\", r, \"F1 Score=\", f)\n",
    "printScores(accuracies, precisions, recalls, f1s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Scores:\n",
      "Avg. accuracy= 0.634076695972\n",
      "Avg. precision= [ 0.82944056  0.34606505  0.52486647]\n",
      "Avg. recall= [ 0.71534605  0.53080357  0.49679012]\n",
      "Avg. f1= [ 0.76792669  0.41861991  0.50997626]\n"
     ]
    }
   ],
   "source": [
    "#Gaussian NB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "accuracies = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "f1s = []\n",
    "\n",
    "for index, (train_ind, test_ind) in enumerate(kf.split(X1, yForSk)):\n",
    "    #print(\"Fold -\", index)\n",
    "    xtrain, xtest = X1[train_ind], X1[test_ind]\n",
    "    ytrain, ytest = list(yForSk[train_ind]), list(yForSk[test_ind])\n",
    "    \n",
    "    gnb = GaussianNB()\n",
    "    #print(X_train.toarray())\n",
    "    gnb.fit(xtrain, ytrain)\n",
    "    #print(log_reg.score(X_train, y_train))\n",
    "    #print(log_reg.score(X_test, y_test))\n",
    "    ypred = gnb.predict(xtest)\n",
    "    acc = np.mean(ytest == ypred)\n",
    "    conf_matrix = confusion_matrix(ytest, ypred)\n",
    "    #print(conf_matrix)\n",
    "    #print(X_train.shape, X_train.shape)\n",
    "    #print(precision_recall_fscore_support(ytest, ypred))\n",
    "    stats = precision_recall_fscore_support(ytest, ypred, labels=['-1', '0', '1'])\n",
    "    \n",
    "    p = np.array([stats[0][2], stats[0][1], stats[0][0]])\n",
    "    r = np.array([stats[1][2], stats[1][1], stats[1][0]])\n",
    "    f = np.array([stats[2][2], stats[2][1], stats[2][0]])\n",
    "    \n",
    "    accuracies.append(acc)\n",
    "    precisions.append(p)\n",
    "    recalls.append(r)\n",
    "    f1s.append(f)\n",
    "    \n",
    "    #print(\"accuracy=\", acc, \"precision=\", p, \"recall=\", r, \"F1 Score=\", f)\n",
    "printScores(accuracies, precisions, recalls, f1s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Scores:\n",
      "Avg. accuracy= 0.710962848482\n",
      "Avg. precision= [ 0.73748585  0.54799839  0.6779456 ]\n",
      "Avg. recall= [ 0.93852193  0.27943948  0.43847222]\n",
      "Avg. f1= [ 0.82590621  0.36888336  0.53149285]\n"
     ]
    }
   ],
   "source": [
    "#Random Forest Classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "accuracies = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "f1s = []\n",
    "\n",
    "for index, (train_ind, test_ind) in enumerate(kf.split(X1, yForSk)):\n",
    "    #print(\"Fold -\", index)\n",
    "    xtrain, xtest = X1[train_ind], X1[test_ind]\n",
    "    ytrain, ytest = list(yForSk[train_ind]), list(yForSk[test_ind])\n",
    "    \n",
    "    rfc = RandomForestClassifier(n_estimators=100, random_state=0)\n",
    "    #print(X_train.toarray())\n",
    "    rfc.fit(xtrain, ytrain)\n",
    "    #print(log_reg.score(X_train, y_train))\n",
    "    #print(log_reg.score(X_test, y_test))\n",
    "    ypred = rfc.predict(xtest)\n",
    "    acc = np.mean(ytest == ypred)\n",
    "    conf_matrix = confusion_matrix(ytest, ypred)\n",
    "    #print(conf_matrix)\n",
    "    #print(X_train.shape, X_train.shape)\n",
    "    #print(precision_recall_fscore_support(ytest, ypred))\n",
    "    stats = precision_recall_fscore_support(ytest, ypred, labels=['-1', '0', '1'])\n",
    "    \n",
    "    p = np.array([stats[0][2], stats[0][1], stats[0][0]])\n",
    "    r = np.array([stats[1][2], stats[1][1], stats[1][0]])\n",
    "    f = np.array([stats[2][2], stats[2][1], stats[2][0]])\n",
    "    \n",
    "    accuracies.append(acc)\n",
    "    precisions.append(p)\n",
    "    recalls.append(r)\n",
    "    f1s.append(f)\n",
    "    \n",
    "    #print(\"accuracy=\", acc, \"precision=\", p, \"recall=\", r, \"F1 Score=\", f)\n",
    "printScores(accuracies, precisions, recalls, f1s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #too slow and 0 neutral class f1 score\n",
    "# from sklearn.svm import SVC\n",
    "\n",
    "# accuracies = []\n",
    "# precisions = []\n",
    "# recalls = []\n",
    "# f1s = []\n",
    "\n",
    "# for index, (train_ind, test_ind) in enumerate(kf.split(X1, yForSk)):\n",
    "#     #print(\"Fold -\", index)\n",
    "#     xtrain, xtest = X1[train_ind], X1[test_ind]\n",
    "#     ytrain, ytest = list(yForSk[train_ind]), list(yForSk[test_ind])\n",
    "    \n",
    "#     clf=SVC(C=1.0, gamma='scale', decision_function_shape='ovr')\n",
    "#     #print(X_train.toarray())\n",
    "#     clf.fit(xtrain, ytrain)\n",
    "#     #print(log_reg.score(X_train, y_train))\n",
    "#     #print(log_reg.score(X_test, y_test))\n",
    "#     ypred = clf.predict(xtest)\n",
    "#     acc = np.mean(ytest == ypred)\n",
    "#     conf_matrix = confusion_matrix(ytest, ypred)\n",
    "#     print(conf_matrix)\n",
    "#     #print(X_train.shape, X_train.shape)\n",
    "#     #print(precision_recall_fscore_support(ytest, ypred))\n",
    "#     stats = precision_recall_fscore_support(ytest, ypred, labels=[-1, 0, 1])\n",
    "    \n",
    "#     p = np.array([stats[0][2], stats[0][1], stats[0][0]])\n",
    "#     r = np.array([stats[1][2], stats[1][1], stats[1][0]])\n",
    "#     f = np.array([stats[2][2], stats[2][1], stats[2][0]])\n",
    "    \n",
    "#     accuracies.append(acc)\n",
    "#     precisions.append(p)\n",
    "#     recalls.append(r)\n",
    "#     f1s.append(f)\n",
    "    \n",
    "#     print(\"accuracy=\", acc, \"precision=\", p, \"recall=\", r, \"F1 Score=\", f)\n",
    "# printScores(accuracies, precisions, recalls, f1s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# testSents = []\n",
    "# testDS = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# with open('Data-1_test.csv') as file:\n",
    "#     file.readline()\n",
    "#     for line in file:\n",
    "#         data = line.split(\",\")\n",
    "#         data[2] = preprocess(data[2])\n",
    "#         data[1] = preprocess(data[1], data[2])\n",
    "#         testSents.append(preprocess(' '.join(data[1]), ' '.join(data[2]), True))\n",
    "#         testDS.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# testXtidf = word2vec.transform(list(np.array(testSents)[:, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# xtest = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for i, data in enumerate(testDS):\n",
    "#     data1 = []\n",
    "#     meanAspVec = None\n",
    "    \n",
    "#     #print(i)\n",
    "#     graph = getSentGraph(testSents[i][0])\n",
    "    \n",
    "#     #if more than one word in aspect term, take mean\n",
    "# #     mean = np.zeros((weD, 1))\n",
    "# #     for w in data[2]:\n",
    "# #         twv = None\n",
    "# #         try:\n",
    "# #             if w in missingVectors:\n",
    "# #                 twv = missingVectors[w]\n",
    "# #             else:\n",
    "# #                 twv = word2vec[w].reshape(weD, 1)\n",
    "# #         except KeyError:\n",
    "# #             twv = np.random.normal(size=(weD, 1))/math.sqrt(weD+1)\n",
    "# #             missingVectors[w] = twv\n",
    "# #         mean += twv\n",
    "# #     meanAspVec = mean/len(data[2])\n",
    "    \n",
    "# #     E = []\n",
    "    \n",
    "# # #     for wv in data1:\n",
    "# # #         E.append(np.dot(wv.T, meanAspVec)/(np.linalg.norm(wv)) * np.linalg.norm(meanAspVec))\n",
    "    \n",
    "# # #     E = np.array(E).reshape(300, 1)\n",
    "# # #     A = np.exp(E) / np.sum(np.exp(E))\n",
    "\n",
    "# #     for w in data[1]:\n",
    "# #         twv = None\n",
    "# #         try:\n",
    "# #             if w in missingVectors:\n",
    "# #                 twv = missingVectors[w]\n",
    "# #             else:\n",
    "# #                 twv = word2vec[w].reshape(weD, 1)\n",
    "# #         except KeyError:\n",
    "# #             twv = np.random.normal(size=(weD, 1))/math.sqrt(weD+1)\n",
    "# #             missingVectors[w] = twv\n",
    "# #         E.append(np.dot(twv.T, meanAspVec) / (np.linalg.norm(twv) * np.linalg.norm(meanAspVec)))\n",
    "\n",
    "# #     A = np.exp(E) / np.sum(np.exp(E)) # softmax\n",
    "    \n",
    "#     tttemp = testSents[i][0].split()\n",
    "#     distVec = []\n",
    "#     for jj, w in enumerate(data[1]):\n",
    "#         if w in testSents[i][1]:\n",
    "#             dist = 0\n",
    "# #         elif w in negWords:\n",
    "# #             if 'not' in w:\n",
    "# #                 dist = 1/nx.shortest_path_length(graph, source=sentences[i][1], target='not-'+str(tttemp.index(w)))\n",
    "# #             elif 'nt' in w:\n",
    "# #                 dist = 1/nx.shortest_path_length(graph, source=sentences[i][1], target='nt-'+str(tttemp.index(w)))\n",
    "# #             elif 'n\\'t' in w:\n",
    "# #                 dist = 1/nx.shortest_path_length(graph, source=sentences[i][1], target='n\\'t-'+str(tttemp.index(w)))\n",
    "#         else:\n",
    "#             try:\n",
    "#                 pD = nx.shortest_path_length(graph, source=testSents[i][1], target=w+'-'+str(tttemp.index(w)))\n",
    "# #                if pD <= 3:\n",
    "#                 dist = 1/pD\n",
    "# #                 else:\n",
    "# #                     dist = 1/maxSentLen\n",
    "#             except (nx.NetworkXNoPath, KeyError, nx.NetworkXError):\n",
    "#                 dist = 1/maxSentLen\n",
    "#         distVec.append(dist)\n",
    "# #         twv = None\n",
    "# #         if w in missingVectors:\n",
    "# #             twv = missingVectors[w]\n",
    "# #         else:\n",
    "# #             twv = word2vec[w].reshape(weD, 1)\n",
    "# #         if flag == \"attention\":\n",
    "# #             data1.append(A[jj] * twv)\n",
    "# #         elif flag == \"input\":\n",
    "# #             data1.append(twv)\n",
    "\n",
    "# #     for w in data[1]:\n",
    "# #         twv = None\n",
    "# #         try:\n",
    "# #             if w in missingVectors:\n",
    "# #                 twv = missingVectors[w]\n",
    "# #             else:\n",
    "# #                 twv = word2vec[w].reshape(300, 1)\n",
    "# #         except KeyError:\n",
    "# #             twv = np.random.normal(size=(300, 1))/math.sqrt(301)\n",
    "# #             missingVectors[w] = twv\n",
    "            \n",
    "# #         data1.append(np.vstack((twv, meanAspVec)))\n",
    "    \n",
    "#     # creating lexicon score with window of 5 to the left and right of aspect\n",
    "#     lexScore = 0\n",
    "#     window = 4\n",
    "#     atPos = int(testSents[i][1].rsplit('-', maxsplit=1)[1])\n",
    "#     for lind in range(atPos - window, atPos):\n",
    "#         if lind >= 0:\n",
    "#             if data[1][lind] in posLex:\n",
    "#                 lexScore += 1\n",
    "#             elif data[1][lind] in negLex or data[1][lind] in negWords:\n",
    "#                 lexScore -= 1\n",
    "    \n",
    "#     for lind in range(atPos + 1, atPos + window + 1):\n",
    "#         if lind >= len(data[1]):\n",
    "#             break\n",
    "#         else:\n",
    "#             if data[1][lind] in posLex:\n",
    "#                 lexScore += 1\n",
    "#             elif data[1][lind] in negLex or data[1][lind] in negWords:\n",
    "#                 lexScore -= 1         \n",
    "\n",
    "                \n",
    "#     if len(data[1]) < maxSentLen:\n",
    "#         j = len(data[1]) + 1\n",
    "#         while j <= maxSentLen:\n",
    "#             #data1.append(np.vstack((word2vec['#'].reshape(300, 1), meanAspVec)))\n",
    "#             #data1.append(np.vstack((np.zeros((300, 1)), meanAspVec)))\n",
    "#             #data1.append(np.zeros((wvD, 1)))\n",
    "#             distVec.append(1/maxSentLen)\n",
    "#             j += 1\n",
    "    \n",
    "#     if len(data[1]) > maxSentLen:\n",
    "#         #del data1[maxSentLen:]\n",
    "#         del distVec[maxSentLen:]\n",
    "        \n",
    "# #     ctr = 0\n",
    "# #     for lind in range(atPos - window, atPos):\n",
    "# #         if lind >= 0:\n",
    "# #             if data[1][lind] in posLex:\n",
    "# #                 lexScore += 1/(window - ctr)\n",
    "# #             elif data[1][lind] in negLex or data[1][lind] in negWords:\n",
    "# #                 lexScore -= 1/(window - ctr)\n",
    "# #         ctr += 1\n",
    "    \n",
    "# #     ctr = 1\n",
    "# #     for lind in range(atPos + 1, atPos + window + 1):\n",
    "# #         if lind >= len(data[1]):\n",
    "# #             break\n",
    "# #         else:\n",
    "# #             if data[1][lind] in posLex:\n",
    "# #                 lexScore += 1/ctr\n",
    "# #             elif data[1][lind] in negLex or data[1][lind] in negWords:\n",
    "# #                 lexScore -= 1/ctr\n",
    "# #         ctr += 1\n",
    "    \n",
    "    \n",
    "#     #protoVec = np.array([np.mean(data1, axis=0)]).reshape(300,)\n",
    "    \n",
    "#     protoVec = np.append(testXtidf[i].A[0], distVec)\n",
    "#     protoVec = np.append(protoVec, [lexScore])\n",
    "    \n",
    "#     xtest.append(protoVec)\n",
    "\n",
    "# xtest = np.array(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# xtest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# clf=LinearSVC(C=1.0, multi_class='crammer_singer', max_iter=10000)\n",
    "# clf.fit(X1, yForSk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ypred = clf.predict(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# with open('testop3.txt', 'a') as ff:\n",
    "#     for i in range(len(testDS)):\n",
    "#         ff.write(testDS[i][0]+\";;\"+ypred[i]+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# accuracies = []\n",
    "# precisions = []\n",
    "# recalls = []\n",
    "# f1s = []\n",
    "\n",
    "# for index, (train_ind, test_ind) in enumerate(kf.split(X1, yForSk)):\n",
    "#     print(\"Fold -\", index)\n",
    "#     xtrain, xtest = X1[train_ind], X1[test_ind]\n",
    "#     ytrain, ytest = y[train_ind], y[test_ind]\n",
    "    \n",
    "#     model = train(xtrain, ytrain)\n",
    "    \n",
    "#     predictions = model.predict(xtest)\n",
    "    \n",
    "#     y_pred = tf.argmax(predictions, dimension=1)\n",
    "#     y_test = tf.argmax(ytest.reshape(len(ytest), 3), dimension=1)\n",
    "    \n",
    "#     print(\"Labels=\", np.unique(y_test, return_counts=True))\n",
    "#     print(\"Predictions=\", np.unique(y_pred, return_counts=True))\n",
    "    \n",
    "#     cm = confusion_matrix(y_test, y_pred)\n",
    "#     numer = np.diag(cm)\n",
    "#     numer = numer.astype(np.float64)\n",
    "#     deno = np.sum(cm, axis = 1, dtype=np.float64)\n",
    "#     recall = np.divide(numer, deno, out=np.zeros_like(numer), where=deno!=0)\n",
    "#     deno = np.sum(cm, axis = 0, dtype=np.float64)\n",
    "#     precision = np.divide(numer, deno, out=np.zeros_like(numer), where=deno!=0)\n",
    "#     #loss, acc = model.evaluate(xtest, ytest)\n",
    "#     nnum = (precision*recall)\n",
    "#     ddeno = (precision+recall)\n",
    "#     f1 = 2*np.divide(nnum, ddeno, out=np.zeros_like(nnum), where=ddeno!=0)\n",
    "#     acc = np.sum(numer)/np.sum(deno)\n",
    "    \n",
    "#     accuracies.append(acc)\n",
    "#     precisions.append(precision)\n",
    "#     recalls.append(recall)\n",
    "#     f1s.append(f1)\n",
    "#     print(\"accuracy=\", acc, \"precision=\", precision, \"recall=\", recall, \"F1 Score=\", f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#word2vec['#']#.reshape(300, 1) #numpy array\n",
    "#print(word2vec.similarity('computer', 'laptop'))\n",
    "#np.zeros(300).shape\n",
    "#word2vec.distances(word2vec['computer'], other_words=[\"laptop\"])\n",
    "#np.append(word2vec[\"###GB\"], np.array([1.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def tteest(text, aspectTerm):\n",
    "#     aspectTerm = aspectTerm.replace(\"[comma]\", \"\").replace(\"  \", \" \").lower()\n",
    "#     text = text.lower()\n",
    "#     x = aspectTerm.replace(\" \",\"_\")\n",
    "#     text = text.replace('\"', '').replace('.','').replace('(',' ').replace('(','').replace(')','').replace('!','').replace(\"?\",'').replace(\"  \",\" \").replace(aspectTerm,x)\n",
    "#     text = text.lstrip('- ')\n",
    "#     text = text.lstrip('_')    \n",
    "#     text = text.lstrip('_ ')\n",
    "#     text = text.lstrip('-')\n",
    "#     text = text.rstrip(' ')\n",
    "#     atPos = text.split().index(x)\n",
    "#     return [text, x+'-'+str(atPos)]\n",
    "\n",
    "\n",
    "# tteest(\"I also enjoy the fact that my MacBook Pro laptop allows me to run Windows 7 on it by using the VMWare program.\",\"VMWare program\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# oma = []\n",
    "# def chamma(line):\n",
    "#     data = line.split(\",\")\n",
    "#     data[2] = preprocess(data[2])\n",
    "#     #print(data[2][0].replace(\"_\", \"\"))\n",
    "#     print(data[2])\n",
    "#     data[1] = preprocess(data[1], data[2])\n",
    "#     print(data[1])\n",
    "#     oma.append(preprocess(' '.join(data[1]), ' '.join(data[2]), True))\n",
    "\n",
    "# chamma(\"1805_1,Good for every day computing and web browsing.,every day computing,9--28,1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# oma[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# corpus = ['This is the first document.',\n",
    "#           'This document is the second document.',\n",
    "#           'And this is the third one.',\n",
    "#           'Is this the first document?']\n",
    "# vectorizer = TfidfVectorizer(ngram_range=(1, 2))\n",
    "# a = vectorizer.fit_transform(corpus)\n",
    "# print(vectorizer.get_feature_names())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
