{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "C:\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import string\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Enable eager execution since this code is experimental\n",
    "tf.enable_eager_execution()\n",
    "tfe = tf.contrib.eager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#table = str.maketrans('/', ' ', '!\"#$%&\\'()*+,-.:;<=>?@[\\\\]^_`{|}~' + \"0123456789\")\n",
    "table2 = str.maketrans('-0123456789', ' LLLLLLLLLL', '\"')\n",
    "stopWords = []\n",
    "dataset = []\n",
    "maxSentLen = 0\n",
    "avgSentLen = 0\n",
    "missingVectors = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word2vec = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n",
    "sentences = []\n",
    "with open('data-1_train.csv') as file:\n",
    "    file.readline()\n",
    "    for line in file:\n",
    "        data = line.split(\",\")\n",
    "#         if data[1][0] == \"\\\"\" and data[1][-1] == \"\\\"\":\n",
    "#             data[1] = data[1][1:-1]\n",
    "        data[1] = data[1].replace(\"[comma]\", \",\").replace(\"  \", \" \").translate(table2)\n",
    "        data[2] = data[2].replace(\"[comma]\", \",\").replace(\"  \", \" \").translate(table2)\n",
    "        temp = list(data[1])\n",
    "        #start, end = data[3].split(\"--\")\n",
    "        start = data[1].index(data[2])\n",
    "        end = start + len(data[2])\n",
    "        for i in range(0, data[2].count(\" \")):\n",
    "            try:\n",
    "                temp[temp.index(\" \", int(start), int(end))] = \"_\"\n",
    "            except ValueError:\n",
    "                pass\n",
    "        sentences.append(gensim.utils.simple_preprocess(''.join(temp), max_len=100))\n",
    "model = gensim.models.Word2Vec(sentences, size=200, window=5, min_count=1, workers=10)\n",
    "word2vec = model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#word2vec['']\n",
    "\"Then just the other day, my left \"\"mouse\"\" button snapped!\".index(\"left \"\"mouse\"\" button\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('stopwords.txt') as file:\n",
    "    for line in file:\n",
    "        line = line.lower()\n",
    "        stopWords.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(text, aspects=None):\n",
    "    #text = text.replace(\"[comma]\", \"\")\n",
    "    #gw = lambda w: w.lower() if w in aspectTerm else w.translate(table).lower()\n",
    "    if aspects:\n",
    "        temp = list(text)\n",
    "        #start, end = aspectTermLoc.split(\"--\")\n",
    "        start = text.index(aspects)\n",
    "        end = start + len(aspects)\n",
    "        for i in range(0, aspects.count(\" \")):\n",
    "            try:\n",
    "                temp[temp.index(\" \", int(start), int(end))] = \"_\"\n",
    "            except ValueError:\n",
    "                pass\n",
    "        text = gensim.utils.simple_preprocess(''.join(temp), max_len=100)\n",
    "        text = [word for word in text if word not in stopWords]        \n",
    "    else:\n",
    "        text = gensim.utils.simple_preprocess(text, max_len=100)#[word.lower() for word in text.translate(table2).split() if len(word) > 1]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# should $numbers be removed ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = 0\n",
    "ll = []\n",
    "with open('data-1_train.csv') as file:\n",
    "    file.readline()\n",
    "    for line in file:\n",
    "        data = line.split(\",\")\n",
    "#         if data[1][0] == \"\\\"\" and data[1][-1] == \"\\\"\":\n",
    "#             data[1] = data[1][1:-1]\n",
    "        data[1] = data[1].replace(\"[comma]\", \",\").replace(\"  \", \" \").translate(table2)\n",
    "        data[2] = data[2].replace(\"[comma]\", \",\").replace(\"  \", \" \").translate(table2)\n",
    "        data[1] = preprocess(data[1], data[2])\n",
    "        temp = list(data[2])\n",
    "        for i in range(0, data[2].count(\" \")):\n",
    "            try:\n",
    "                temp[temp.index(\" \")] = \"_\"\n",
    "            except ValueError:\n",
    "                pass\n",
    "        data[2] = preprocess(''.join(temp))\n",
    "        data[-1] = data[-1].strip()\n",
    "        length += len(data[1])\n",
    "        ll.append(len(data[1]))\n",
    "        if len(data[1]) > maxSentLen:\n",
    "            maxSentLen = len(data[1])\n",
    "        dataset.append(data)\n",
    "avgSentLen = length / len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33 7.642023346303502\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "print(maxSentLen, avgSentLen)\n",
    "print(sorted(ll)[int(len(ll)/2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1d16a922a90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(len(ll)), sorted(ll))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "maxSentLen = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weD = 200\n",
    "wvD = 2*weD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(data, label):\n",
    "    inputs = keras.layers.Input(shape=(maxSentLen, wvD, 1))\n",
    "    #print(inputs.shape)\n",
    "\n",
    "    #layer<num>_<filtersize>\n",
    "    conv_layer1_2 = keras.layers.Conv2D(200, (3, wvD), 1, padding='valid', activation=tf.nn.relu)(inputs)\n",
    "    #print(conv_layer1_2.shape)\n",
    "    pool_layer1_2 = keras.layers.MaxPool2D((maxSentLen-3+1, 1), strides=1, padding=\"valid\")(conv_layer1_2)\n",
    "    #print(pool_layer1_2.shape)\n",
    "\n",
    "    conv_layer1_3 = keras.layers.Conv2D(200, (4, wvD), 1, padding='valid', activation=tf.nn.relu)(inputs)\n",
    "    #print(conv_layer1_3.shape)\n",
    "    pool_layer1_3 = keras.layers.MaxPool2D((maxSentLen-4+1, 1), strides=1, padding=\"valid\")(conv_layer1_3)\n",
    "    #print(pool_layer1_3.shape)\n",
    "\n",
    "    conv_layer1_4 = keras.layers.Conv2D(200, (5, wvD), 1, padding='valid', activation=tf.nn.relu)(inputs)\n",
    "    #print(conv_layer1_4.shape)\n",
    "    pool_layer1_4 = keras.layers.MaxPool2D((maxSentLen-5+1, 1), strides=1, padding=\"valid\")(conv_layer1_4)\n",
    "    #print(pool_layer1_4.shape)\n",
    "\n",
    "    layer1 = keras.layers.concatenate([pool_layer1_2, pool_layer1_3, pool_layer1_4], axis=1)\n",
    "    #print(layer1.shape)\n",
    "    layer1 = keras.layers.Flatten()(layer1)\n",
    "    #print(layer1.shape)\n",
    "    \n",
    "    dropout = keras.layers.Dropout(0.5)(layer1)\n",
    "    \n",
    "    out = keras.layers.Dense(3, activation=tf.nn.softmax, kernel_regularizer=keras.regularizers.l2(0.01))(dropout)\n",
    "    #print(out.shape)\n",
    "\n",
    "    nnModel = keras.models.Model(inputs=inputs, outputs=out)\n",
    "    nnModel.compile(optimizer=tf.train.AdamOptimizer(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    #data = np.array([[word2vec['computer'].reshape(300, 1), word2vec['good'].reshape(300, 1), word2vec['screen'].reshape(300, 1), word2vec['bad'].reshape(300, 1), word2vec['keyboard'].reshape(300, 1)]])\n",
    "    #print(\"data=\", data.shape)\n",
    "    #label = np.array([[1.0, 0.0, 0.0]])\n",
    "    #print(\"label=\", label.shape)\n",
    "    nnModel.fit(data, label, epochs=5)\n",
    "    return nnModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X1 = []\n",
    "y = []\n",
    "yForSk = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for iii, data in enumerate(dataset):\n",
    "    data1 = []\n",
    "    meanAspVec = None\n",
    "    \n",
    "    #if more than one word in aspect term, take mean\n",
    "    #mean = np.zeros((weD, 1))\n",
    "    for w in data[2]:\n",
    "        twv = None\n",
    "        try:\n",
    "            if w in missingVectors:\n",
    "                twv = missingVectors[w]\n",
    "            else:\n",
    "                twv = word2vec[w].reshape(weD, 1)\n",
    "        except KeyError:\n",
    "            twv = np.random.normal(size=(weD, 1))/math.sqrt(weD+1)\n",
    "            missingVectors[w] = twv\n",
    "        meanAspVec = twv\n",
    "    #print(iii, \"---\" ,data[2])\n",
    "#     try:\n",
    "#         print(word2vec[data[2]].shape)\n",
    "#     except KeyError:\n",
    "#         print(\"KeyError---###################\", iii)\n",
    "    #meanAspVec = word2vec[data[2]].reshape(weD, 1)#mean/len(data[2])\n",
    "    \n",
    "    E = []\n",
    "    \n",
    "#     for wv in data1:\n",
    "#         E.append(np.dot(wv.T, meanAspVec)/(np.linalg.norm(wv)) * np.linalg.norm(meanAspVec))\n",
    "    \n",
    "#     E = np.array(E).reshape(300, 1)\n",
    "#     A = np.exp(E) / np.sum(np.exp(E))\n",
    "\n",
    "    for w in data[1]:\n",
    "        twv = None\n",
    "        try:\n",
    "            if w in missingVectors:\n",
    "                twv = missingVectors[w]\n",
    "            else:\n",
    "                twv = word2vec[w].reshape(weD, 1)\n",
    "        except KeyError:\n",
    "            twv = np.random.normal(size=(weD, 1))/math.sqrt(weD+1)\n",
    "            missingVectors[w] = twv\n",
    "        E.append(np.dot(twv.T, meanAspVec) / (np.linalg.norm(twv) * np.linalg.norm(meanAspVec)))\n",
    "\n",
    "    A = np.exp(E) / np.sum(np.exp(E)) # softmax\n",
    "\n",
    "    for i, w in enumerate(data[1]):\n",
    "        twv = None\n",
    "        if w in missingVectors:\n",
    "            twv = missingVectors[w]\n",
    "        else:\n",
    "            twv = word2vec[w].reshape(weD, 1)\n",
    "        data1.append(np.vstack((twv, A[i] * twv)))\n",
    "\n",
    "#     for w in data[1]:\n",
    "#         twv = None\n",
    "#         try:\n",
    "#             if w in missingVectors:\n",
    "#                 twv = missingVectors[w]\n",
    "#             else:\n",
    "#                 twv = word2vec[w].reshape(300, 1)\n",
    "#         except KeyError:\n",
    "#             twv = np.random.normal(size=(300, 1))/math.sqrt(301)\n",
    "#             missingVectors[w] = twv\n",
    "            \n",
    "#         data1.append(np.vstack((twv, meanAspVec)))\n",
    "    \n",
    "    if len(data1) < maxSentLen:\n",
    "        j = len(data1) + 1\n",
    "        while j <= maxSentLen:\n",
    "            #data1.append(np.vstack((word2vec['#'].reshape(300, 1), meanAspVec)))\n",
    "            #data1.append(np.vstack((np.zeros((300, 1)), meanAspVec)))\n",
    "            data1.append(np.zeros((wvD, 1)))\n",
    "            j += 1\n",
    "    \n",
    "    if len(data1) > maxSentLen:\n",
    "        del data1[maxSentLen:]\n",
    "    \n",
    "    X1.append(np.array(data1))\n",
    "    yForSk.append(data[-1])\n",
    "    if data[-1] == '-1':\n",
    "        y.append(np.array([[0.0, 0.0, 1.0]]))\n",
    "    elif data[-1] == '0':\n",
    "        y.append(np.array([[0.0, 1.0, 0.0]]))\n",
    "    elif data[-1] == '1':\n",
    "        y.append(np.array([[1.0, 0.0, 0.0]]))\n",
    "\n",
    "X1 = np.array(X1)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2313, 15, 400, 1)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2313, 1, 3)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2313,)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(yForSk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kf = StratifiedKFold(n_splits=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold - 0\n",
      "Epoch 1/5\n",
      "66/66 [==============================] - 4s 58ms/step - loss: 11.3367 - acc: 0.3983\n",
      "Epoch 2/5\n",
      "66/66 [==============================] - 4s 57ms/step - loss: 11.1821 - acc: 0.4135 0s - los\n",
      "Epoch 3/5\n",
      "66/66 [==============================] - 5s 76ms/step - loss: 11.1485 - acc: 0.4185\n",
      "Epoch 4/5\n",
      "66/66 [==============================] - 4s 65ms/step - loss: 11.1387 - acc: 0.4204\n",
      "Epoch 5/5\n",
      "66/66 [==============================] - 4s 54ms/step - loss: 11.1264 - acc: 0.4181\n",
      "Labels= (array([0, 1, 2], dtype=int64), array([99, 46, 87], dtype=int64))\n",
      "Predictions= (array([0], dtype=int64), array([232], dtype=int64))\n",
      "accuracy= 0.4267241379310345 precision= [0.42672414 0.         0.        ] recall= [1. 0. 0.] F1 Score= [0.59818731 0.         0.        ]\n",
      "Fold - 1\n",
      "Epoch 1/5\n",
      "66/66 [==============================] - 3s 53ms/step - loss: 11.2722 - acc: 0.4059\n",
      "Epoch 2/5\n",
      "66/66 [==============================] - 3s 53ms/step - loss: 11.1744 - acc: 0.4182\n",
      "Epoch 3/5\n",
      "66/66 [==============================] - 3s 51ms/step - loss: 11.1479 - acc: 0.4204\n",
      "Epoch 4/5\n",
      "66/66 [==============================] - 4s 60ms/step - loss: 11.1309 - acc: 0.4201\n",
      "Epoch 5/5\n",
      "66/66 [==============================] - 4s 57ms/step - loss: 11.1202 - acc: 0.4205\n",
      "Labels= (array([0, 1, 2], dtype=int64), array([99, 46, 87], dtype=int64))\n",
      "Predictions= (array([0], dtype=int64), array([232], dtype=int64))\n",
      "accuracy= 0.4267241379310345 precision= [0.42672414 0.         0.        ] recall= [1. 0. 0.] F1 Score= [0.59818731 0.         0.        ]\n",
      "Fold - 2\n",
      "Epoch 1/5\n",
      "66/66 [==============================] - 5s 70ms/step - loss: 11.2963 - acc: 0.4145\n",
      "Epoch 2/5\n",
      "59/66 [=========================>....] - ETA: 0s - loss: 11.1105 - acc: 0.4149"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-d6e993097c2f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mytrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mytest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_ind\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest_ind\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mytrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-15-4947b8133438>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(data, label)\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[1;31m#label = np.array([[1.0, 0.0, 0.0]])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[1;31m#print(\"label=\", label.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m     \u001b[0mnnModel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mnnModel\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1577\u001b[0m           \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1578\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1579\u001b[1;33m           validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1580\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_distribution_strategy\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1581\u001b[0m       return training_distributed.fit_loop(\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_eager.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, inputs, targets, sample_weights, class_weight, val_inputs, val_targets, val_sample_weights, batch_size, epochs, verbose, callbacks, shuffle, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    696\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    697\u001b[0m           \u001b[0mdo_validation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdo_validation\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 698\u001b[1;33m           batch_size=batch_size)\n\u001b[0m\u001b[0;32m    699\u001b[0m       \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    700\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_eager.py\u001b[0m in \u001b[0;36miterator_fit_loop\u001b[1;34m(model, inputs, class_weight, steps_per_epoch, epoch_logs, val_inputs, val_targets, val_sample_weights, epochs, verbose, callbacks, validation_steps, do_validation, batch_size)\u001b[0m\n\u001b[0;32m    248\u001b[0m     \u001b[1;31m# Train model.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    249\u001b[0m     outs, loss, loss_metrics, masks = _process_single_batch(\n\u001b[1;32m--> 250\u001b[1;33m         model, x, y, sample_weights=sample_weights, training=True)\n\u001b[0m\u001b[0;32m    251\u001b[0m     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgeneric_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    252\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_eager.py\u001b[0m in \u001b[0;36m_process_single_batch\u001b[1;34m(model, inputs, targets, sample_weights, training)\u001b[0m\n\u001b[0;32m    514\u001b[0m         \u001b[0mgrads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_collected_trainable_weights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    515\u001b[0m         model.optimizer.apply_gradients(zip(grads,\n\u001b[1;32m--> 516\u001b[1;33m                                             model._collected_trainable_weights))\n\u001b[0m\u001b[0;32m    517\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_metrics\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    518\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\optimizers.py\u001b[0m in \u001b[0;36mapply_gradients\u001b[1;34m(self, grads)\u001b[0m\n\u001b[0;32m    704\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    705\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mapply_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 706\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterations\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    707\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    708\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mget_grads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\optimizer.py\u001b[0m in \u001b[0;36mapply_gradients\u001b[1;34m(self, grads_and_vars, global_step, name)\u001b[0m\n\u001b[0;32m    606\u001b[0m           \u001b[0mscope_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    607\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"update_\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mscope_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 608\u001b[1;33m           \u001b[0mupdate_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    609\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mglobal_step\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    610\u001b[0m         \u001b[0mapply_updates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_finish\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mupdate_ops\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\optimizer.py\u001b[0m in \u001b[0;36mupdate_op\u001b[1;34m(self, optimizer, g)\u001b[0m\n\u001b[0;32m    168\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_v\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstraint\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrol_dependencies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mupdate_op\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 170\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_v\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massign\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_v\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_v\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    171\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mupdate_op\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py\u001b[0m in \u001b[0;36massign\u001b[1;34m(self, value, use_locking, name, read_value)\u001b[0m\n\u001b[0;32m    979\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_shape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massert_is_compatible_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue_tensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    980\u001b[0m       assign_op = gen_resource_variable_ops.assign_variable_op(\n\u001b[1;32m--> 981\u001b[1;33m           self.handle, value_tensor, name=name)\n\u001b[0m\u001b[0;32m    982\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mread_value\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    983\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lazy_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0massign_op\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_resource_variable_ops.py\u001b[0m in \u001b[0;36massign_variable_op\u001b[1;34m(resource, value, name)\u001b[0m\n\u001b[0;32m    168\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_eager_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m         \u001b[1;34m\"AssignVariableOp\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_post_execution_callbacks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresource\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 170\u001b[1;33m         value)\n\u001b[0m\u001b[0;32m    171\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "accuracies = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "f1s = []\n",
    "\n",
    "for index, (train_ind, test_ind) in enumerate(kf.split(X1, yForSk)):\n",
    "    print(\"Fold -\", index)\n",
    "    xtrain, xtest = X1[train_ind], X1[test_ind]\n",
    "    ytrain, ytest = y[train_ind], y[test_ind]\n",
    "    \n",
    "    model = train(xtrain, ytrain)\n",
    "    \n",
    "    predictions = model.predict(xtest)\n",
    "    \n",
    "    y_pred = tf.argmax(predictions, dimension=1)\n",
    "    y_test = tf.argmax(ytest.reshape(len(ytest), 3), dimension=1)\n",
    "    \n",
    "    print(\"Labels=\", np.unique(y_test, return_counts=True))\n",
    "    print(\"Predictions=\", np.unique(y_pred, return_counts=True))\n",
    "    \n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    numer = np.diag(cm)\n",
    "    numer = numer.astype(np.float64)\n",
    "    deno = np.sum(cm, axis = 1, dtype=np.float64)\n",
    "    recall = np.divide(numer, deno, out=np.zeros_like(numer), where=deno!=0)\n",
    "    deno = np.sum(cm, axis = 0, dtype=np.float64)\n",
    "    precision = np.divide(numer, deno, out=np.zeros_like(numer), where=deno!=0)\n",
    "    #loss, acc = model.evaluate(xtest, ytest)\n",
    "    nnum = (precision*recall)\n",
    "    ddeno = (precision+recall)\n",
    "    f1 = 2*np.divide(nnum, ddeno, out=np.zeros_like(nnum), where=ddeno!=0)\n",
    "    acc = np.sum(numer)/np.sum(deno)\n",
    "    \n",
    "    accuracies.append(acc)\n",
    "    precisions.append(precision)\n",
    "    recalls.append(recall)\n",
    "    f1s.append(f1)\n",
    "    print(\"accuracy=\", acc, \"precision=\", precision, \"recall=\", recall, \"F1 Score=\", f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Average Scores:\")\n",
    "print(\"Avg. accuracy=\", np.sum(accuracies)/len(accuracies))\n",
    "print(\"Avg. precision=\", np.sum(precisions, axis=0)/len(precisions))\n",
    "print(\"Avg. recall=\", np.sum(recalls, axis=0)/len(recalls))\n",
    "print(\"Avg. f1=\", np.sum(f1s, axis=0)/len(f1s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#pred = tf.one_hot(tf.argmax(predictions, dimension=1), depth=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[475]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word2vec['#']#.reshape(300, 1) #numpy array\n",
    "#print(word2vec.similarity('computer', 'laptop'))\n",
    "#np.zeros(300).shape\n",
    "#word2vec.distances(word2vec['computer'], other_words=[\"laptop\"])\n",
    "# a = list('I preferred the fit and feel of the $$ inch.')\n",
    "# a[a.index(\" \", 37, 44)] = \"_\"\n",
    "# #a[a.index(\" \", 36, 43)] = \"_\"\n",
    "# print(''.join(a))\n",
    "gensim.utils.simple_preprocess('built_in')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
